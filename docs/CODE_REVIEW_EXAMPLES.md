# ğŸ¬ Exemplos PrÃ¡ticos: Code Review da Skill em AÃ§Ã£o

## CenÃ¡rio 1: Revisar Script de AvaliaÃ§Ã£o (LangChain Integration)

### Comando
```bash
python src/code_review.py src/evaluate.py
```

### Resultado Esperado (Exemplo)
```
================================================================================
ğŸ” CODE REVIEW REPORT
================================================================================

ğŸ“Š RESUMO
  - Critical:  0 issue(ns)
  - High:      2 issue(ns)
  - Medium:    1 issue(ns)
  - Low:       0 issue(ns)
  - Suggestion: 0 issue(ns)
  - TOTAL:     3 issue(ns)


ğŸŸ  HIGH (2 issues)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“„ src\evaluate.py:120
   Categoria: LangChain Integration
   TÃ­tulo: hub.pull sem tratamento de erro
   DescriÃ§Ã£o: hub.pull() pode falhar se prompt nÃ£o existir
   CÃ³digo: prompt = hub.pull(prompt_name)
   âœ¨ SugestÃ£o: Envolver em try-except com mensagem de erro clara

ğŸ“„ src\evaluate.py:175
   Categoria: LangChain Integration
   TÃ­tulo: Cliente LangSmith sem validaÃ§Ã£o
   DescriÃ§Ã£o: Deve validar LANGSMITH_API_KEY antes
   âœ¨ SugestÃ£o: Adicionar check_env_vars() antes de Client()


ğŸŸ¡ MEDIUM (1 issues)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“„ src\evaluate.py:280
   Categoria: Documentation
   TÃ­tulo: Docstring ausente
   DescriÃ§Ã£o: FunÃ§Ã£o sem documentaÃ§Ã£o
   âœ¨ SugestÃ£o: Adicionar docstring no formato Google/NumPy

================================================================================
âœ… RelatÃ³rio salvo em: code_review_report.json
```

### Como Corrigir - Hub Pull Sem Try-Except

**Antes (Problema):**
```python
def load_prompt_from_hub(prompt_name: str):
    prompt = hub.pull(prompt_name)  # âŒ Pode falhar
    return prompt
```

**Depois (Corrigido):**
```python
def load_prompt_from_hub(prompt_name: str) -> RunnableSequence:
    """
    Carrega prompt do LangSmith Hub com validaÃ§Ã£o de erro.
    
    Args:
        prompt_name: Nome do prompt (formato: "username/prompt_v2")
    
    Returns:
        RunnableSequence do LangChain
    
    Raises:
        ValueError: Se prompt nÃ£o encontrada no Hub
    """
    try:
        prompt = hub.pull(prompt_name)
        logger.info(f"Prompt '{prompt_name}' carregado com sucesso")
        return prompt
    except Exception as e:
        error_msg = f"Falha ao carregar prompt '{prompt_name}' do Hub: {e}"
        logger.error(error_msg)
        raise ValueError(error_msg) from e
```

---

### Como Corrigir - ValidaÃ§Ã£o de Credenciais

**Antes:**
```python
client = Client()  # âŒ Pode falhar silenciosamente
```

**Depois:**
```python
# 1. Adicionar funÃ§Ã£o helper em utils.py
def validate_langsmith_credentials() -> Client:
    """Valida e retorna cliente LangSmith autenticado."""
    api_key = os.getenv("LANGSMITH_API_KEY")
    if not api_key:
        raise ValueError(
            "LANGSMITH_API_KEY nÃ£o configurada.\n"
            "1. Crie chave em: https://smith.langchain.com/settings/keys\n"
            "2. Adicione ao .env: LANGSMITH_API_KEY=lsv2_pt_...\n"
            "3. Reinicie a aplicaÃ§Ã£o"
        )
    return Client()

# 2. Usar no cÃ³digo
try:
    client = validate_langsmith_credentials()
    logger.info("LangSmith cliente autenticado")
except ValueError as e:
    logger.error(f"Erro de autenticaÃ§Ã£o: {e}")
    sys.exit(1)
```

---

## CenÃ¡rio 2: Revisar Novo Prompt YAML

### Comando
```bash
python src/code_review.py prompts/bug_to_user_story_v2.yml
```

### Resultado Esperado (Exemplo)
```
================================================================================
ğŸ” CODE REVIEW REPORT
================================================================================

ğŸ“Š RESUMO
  - Critical:  1 issue(ns)
  - High:      1 issue(ns)
  - Medium:    0 issue(ns)
  - Low:       0 issue(ns)
  - Suggestion: 0 issue(ns)
  - TOTAL:     2 issue(ns)


ğŸ”´ CRITICAL (1 issues)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“„ prompts\bug_to_user_story_v2.yml
   Categoria: Prompt Structure
   TÃ­tulo: Campo obrigatÃ³rio ausente: 'techniques_applied'
   DescriÃ§Ã£o: Prompt YAML deve conter: system_prompt, version, techniques_applied, description
   âœ¨ SugestÃ£o: Adicionar campo 'techniques_applied' ao prompt YAML


ğŸŸ  HIGH (1 issues)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“„ prompts\bug_to_user_story_v2.yml
   Categoria: Few-Shot Learning
   TÃ­tulo: Few-shot examples insuficientes
   DescriÃ§Ã£o: Recomenda-se 2-3 exemplos para melhor performance
   âœ¨ SugestÃ£o: Expandir exemplos para cobrir casos comuns e edge cases

================================================================================
âœ… RelatÃ³rio salvo em: code_review_report.json
```

### Como Corrigir - Adicionar TÃ©cnicas

**Antes:**
```yaml
version: v2
system_prompt: "VocÃª Ã© um Product Manager..."
description: "Converte bugs em user stories"
# âŒ Ausente: techniques_applied
```

**Depois:**
```yaml
version: v2
system_prompt: "VocÃª Ã© um Product Manager..."
description: "Converte bugs em user stories usando tÃ©cnicas avanÃ§adas"

techniques_applied:
  - "Few-shot Learning"           # 2-3 exemplos claros de input/output
  - "Chain of Thought"             # Instruir modelo a pensar passo a passo
  - "Role Prompting"               # Definir persona detalhada

examples:
  - name: "Simple bug fix"
    input: "Button not clickable on mobile"
    output: |
      ## User Story
      As a mobile user
      I want clickable buttons
      So my interactions are processed
      
      ## Acceptance Criteria
      - [ ] Button responds to touch within 50ms
      - [ ] Works on iOS 14+, Android 11+
```

---

### Como Corrigir - Adicionar Few-Shot Examples

**Antes:**
```yaml
examples:
  - input: "Fix authentication bug"
    output: "User Story format..."
# âŒ Apenas 1 exemplo, sem clareza
```

**Depois:**
```yaml
examples:
  # Exemplo 1: Bug Simples (Happy Path)
  - name: "Simple Critical Bug"
    input: "Login button is broken on homepage"
    output: |
      ## User Story
      As a user trying to login
      I want the login button to be clickable
      So I can access my account
      
      ## Acceptance Criteria
      - [ ] Button is visible and enabled
      - [ ] Clicking navigates to login page
      - [ ] Works on Chrome, Firefox, Safari
      
      ## Tasks
      - [ ] Check button HTML elements
      - [ ] Verify onClick handler
      - [ ] Update CSS if needed

  # Exemplo 2: Bug AmbÃ­guo (Com ClarificaÃ§Ãµes)
  - name: "Ambiguous Performance Issue"
    input: "Dashboard is very slow"
    output: |
      ## User Story (Clarifications Needed)
      As a power user viewing the dashboard
      I want the dashboard to load in < 2 seconds
      So I can quickly monitor system status
      
      ## Acceptance Criteria
      - [ ] Initial load: < 2000ms
      - [ ] Interactions responsive: < 500ms
      - [ ] Works with 1000+ data points
      - [ ] Tested on Chrome DevTools throttling
      
      **Questions to Ask:**
      - Which dashboard view is slow? (list, charts, etc)
      - What data volume? (small test, production)
      - Browser/device constraints?

  # Exemplo 3: Bug Complexo (Refactoring)
  - name: "Complex Architectural Change"
    input: "Rewrite authentication module to use OAuth2"
    output: |
      ## User Story
      As a security-conscious user
      I want OAuth2 authentication
      So my credentials are protected and shared safely
      
      ## Acceptance Criteria
      - [ ] OAuth2 flow implemented (Google, GitHub)
      - [ ] Legacy auth methods deprecated
      - [ ] User migration script created
      - [ ] All tests passing (coverage > 90%)
      - [ ] Security audit completed
      
      ## Tasks
      - [ ] Design OAuth2 integration
      - [ ] Implement provider connectors
      - [ ] Create migration job
      - [ ] Update documentation
      
      ## Risks
      - User migration downtime
      - Third-party service dependency
      - Legacy system compatibility
```

---

## CenÃ¡rio 3: Revisar Testes

### Comando
```bash
python src/code_review.py tests/test_prompts.py
```

### Resultado Esperado (Exemplo)
```
================================================================================
ğŸ” CODE REVIEW REPORT
================================================================================

ğŸ“Š RESUMO
  - Critical:  0 issue(ns)
  - High:      0 issue(ns)
  - Medium:    0 issue(ns)
  - Low:       1 issue(ns)
  - Suggestion: 0 issue(ns)
  - TOTAL:     1 issue(ns)


ğŸŸ¢ LOW (1 issues)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“„ tests\test_prompts.py:45
   Categoria: Type Hints
   TÃ­tulo: Type hints ausentes
   DescriÃ§Ã£o: FunÃ§Ã£o deveria ter anotaÃ§Ãµes de tipo
   âœ¨ SugestÃ£o: Adicionar type hints: def func(param: str) -> dict:

================================================================================
âœ… Todos os testes implementados corretamente!
âœ… RelatÃ³rio salvo em: code_review_report.json
```

---

## CenÃ¡rio 4: Revisar Todo RepositÃ³rio (Full Scan)

### Comando
```bash
python src/code_review.py .
```

### Resultado Esperado (Exemplo Resumido)
```
================================================================================
ğŸ” CODE REVIEW REPORT - FULL REPOSITORY SCAN
================================================================================

ğŸ“Š RESUMO GERAL
  - Critical:  0 issue(ns)
  - High:      2 issue(ns)
  - Medium:    5 issue(ns)
  - Low:       8 issue(ns)
  - Suggestion: 3 issue(ns)
  - TOTAL:     18 issue(ns)

ğŸ“Š BREAKDOWN POR ARQUIVO
  - src/evaluate.py:        2 HIGH, 1 MEDIUM
  - src/metrics.py:         1 MEDIUM, 2 LOW
  - src/utils.py:           4 LOW, 1 SUGGESTION
  - prompts/*.yml:          2 MEDIUM, 3 SUGGESTION
  - tests/test_prompts.py:  2 LOW

ğŸ¯ AÃ‡ÃƒO IMEDIATA
  1. Revisar 2 issues HIGH em src/evaluate.py
  2. Implementar fixes e re-rodar
  
âœ… RelatÃ³rio salvo em: code_review_report.json
```

---

## CenÃ¡rio 5: IntegraÃ§Ã£o em GitHub Actions (AutomÃ¡tica)

### Setup
```bash
# 1. Criar arquivo
touch .github/workflows/code-review-ci.yml

# 2. Copiar conteÃºdo do template (veja code-review-skill.md)

# 3. Fazer commit
git add .github/workflows/code-review-ci.yml
git commit -m "CI: Add automated code review"
```

### Resultado ao Fazer PR
```
PR #42: "Feat: Improve prompt evaluation"

âœ… Checks
â”œâ”€ Code Review        PASS
â”œâ”€ Security Scan      PASS
â”œâ”€ Unit Tests         PASS (45/45)
â”œâ”€ Coverage           PASS (86%)
â””â”€ All checks passed

ğŸ“ Comment by GitHub Actions:

## ğŸ” Code Review Results
```
$REPORT_CONTENT
```

:heavy_check_mark: APPROVED - Ready to merge!
```

---

## CenÃ¡rio 6: Corrigir SeguranÃ§a (API Keys)

### Problema Detectado
```
python src/code_review.py src/

ğŸ”´ CRITICAL: Hardcoded OpenAI API key detected
   Arquivo: src/config_provider.py (linha 25)
   CÃ³digo: api_key = "sk-proj-xxxxx"
   RISCO: Chave exposta em repositÃ³rio pÃºblico!
```

### SoluÃ§Ã£o Passo a Passo

**Passo 1: Criar .env (se nÃ£o existir)**
```bash
cp .env.example .env
# Editar .env com suas chaves (jÃ¡ estÃ¡ em .gitignore)
```

**Passo 2: Refatorar cÃ³digo**
```python
# Antes (ERRADO)
openai.api_key = "sk-proj-xxxxx"

# Depois (CORRETO)
import os
from dotenv import load_dotenv

load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# Com validaÃ§Ã£o
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEY nÃ£o configurada no .env")
```

**Passo 3: Revogar chaves comprometidas**
```
1. OpenAI: https://platform.openai.com/account/api-keys
2. Google: https://aistudio.google.com/app/apikey
3. LangSmith: https://smith.langchain.com/settings/keys
```

**Passo 4: Gerar chaves novas e atualizar .env**

**Passo 5: Verificar se foi removida do histÃ³rico git**
```bash
# Verificar se a chave aparece em commits anteriores
git log -p | grep "sk-proj-\|AIzaSy"

# Se aparecer, usar BFG Repo-Cleaner
# (https://rtyley.github.io/bfg-repo-cleaner/)
```

**Passo 6: Re-rodar code review**
```bash
python src/code_review.py src/

# Esperado: âœ… Sem CRITICAL issues
```

---

## CenÃ¡rio 7: Melhorar DocumentaÃ§Ã£o

### Problema Detectado
```
ğŸ“„ src/evaluate.py:85
   Categoria: Documentation
   TÃ­tulo: Docstring ausente
   DescriÃ§Ã£o: FunÃ§Ã£o 'calculate_metrics' sem documentaÃ§Ã£o
```

### SoluÃ§Ã£o

**Antes (sem docstring):**
```python
def calculate_metrics(prompt_output, expected_output, evaluator_llm):
    # Calcula mÃ©tricas
    f1_score = 2 * (precision * recall) / (precision + recall)
    return {"f1": f1_score, "clarity": 0.8}
```

**Depois (com docstring completa):**
```python
def calculate_metrics(
    prompt_output: str,
    expected_output: str,
    evaluator_llm: BaseLanguageModel
) -> Dict[str, float]:
    """
    Calcula mÃ©tricas de avaliaÃ§Ã£o do prompt usando LLM como juiz.
    
    Implementa avaliaÃ§Ã£o multi-dimensional baseada em:
    - F1-Score: PrecisÃ£o e recall da resposta
    - Clarity: Qualidade e estrutura da resposta
    - Tone: Tom e profissionalismo apropriado
    - Completeness: Cobertura de critÃ©rios de aceitaÃ§Ã£o
    
    Args:
        prompt_output: Resposta gerada pelo modelo
        expected_output: Output esperado (ground truth)
        evaluator_llm: LLM para fazer avaliaÃ§Ã£o (ex: GPT-4)
    
    Returns:
        Dict com mÃ©tricas normalizadas [0, 1]:
        {
            "f1_score": float,
            "clarity": float,
            "tone": float,
            "completeness": float
        }
    
    Raises:
        ValueError: Se outputs vazios ou LLM falha
    
    Example:
        >>> metrics = calculate_metrics(
        ...     prompt_output="User story...",
        ...     expected_output="User story...",
        ...     evaluator_llm=ChatOpenAI()
        ... )
        >>> print(metrics["f1_score"])
        0.92
    """
    # ImplementaÃ§Ã£o...
    return {"f1": f1_score, "clarity": 0.8}
```

---

## CenÃ¡rio 8: Daily Workflow com Code Review

### Morning: ComeÃ§ar Feature

```bash
# 1. Atualizar repositÃ³rio
git checkout main && git pull

# 2. Criar branch
git checkout -b feat/improve-prompts

# 3. Ativar environment
source venv/bin/activate  # Windows: venv\Scripts\activate

# 4. ComeÃ§ar desenvolvimento
# editar arquivo X
```

### During: Testar AlteraÃ§Ãµes

```bash
# 1. Rodar testes locais
pytest tests/ -v

# 2. Testar funcionalidade
python src/evaluate.py

# 3. Code review local
python src/code_review.py src/
python src/code_review.py prompts/

# 4. Revisar saÃ­da
# Corrigir issues se houver
```

### Evening: Fazer Commit

```bash
# 1. Verificar mudanÃ§as
git diff

# 2. SeguranÃ§a final
grep -r "sk-proj-\|AIzaSy\|lsv2_pt_" src/ prompts/ tests/ && echo "ISSUES!" || echo "âœ… OK"

# 3. Testes finais
pytest tests/ -v --cov=src

# 4. Code review final
python src/code_review.py .

# 5. Se tudo ok
git add .
git commit -m "feat: Implement improved prompts with better examples"
git push origin feat/improve-prompts
```

### Results: GitHub Actions AutomÃ¡tico

```
PR criado â†’ GitHub Actions executa:
1. âœ… Code Review automÃ¡tico
2. âœ… Testes com coverage
3. âœ… Security scan
4. âœ… ComentÃ¡rio com resultados

Se tudo PASS â†’ VocÃª aprova e faz merge
```

---

## ğŸ¯ Resumo dos CenÃ¡rios

| CenÃ¡rio | Comando | Tempo | BenefÃ­cio |
|---------|---------|-------|-----------|
| **1. Script Python** | `code_review.py src/evaluate.py` | 5s | Issues LangChain |
| **2. YAML Prompt** | `code_review.py prompts/*.yml` | 5s | ValidaÃ§Ã£o Estrutura |
| **3. Testes** | `code_review.py tests/` | 3s | Coverage feedback |
| **4. Scan Completo** | `code_review.py .` | 15s | Overview geral |
| **5. GitHub Actions** | Push PR | Auto | AutomÃ¡tico |
| **6. SeguranÃ§a** | `code_review.py src/` | < 1s | API keys detectadas |
| **7. DocumentaÃ§Ã£o** | `code_review.py src/` | < 1s | Docstrings |
| **8. Daily Workflow** | Integrado | Normal | Fluxo naturalizado |

---

## âœ… Checklist: Skill em AÃ§Ã£o

Quando terminar desenvolvimento:

- [ ] Rodei `python src/code_review.py src/`
- [ ] Rodei `python src/code_review.py prompts/`
- [ ] Arrumei issues HIGH/CRITICAL
- [ ] Testes passam: `pytest tests/ -v`
- [ ] Sem secrets expostas: `grep -r "sk-proj"` = vazio
- [ ] DocumentaÃ§Ã£o atualizada: Docstrings presentes
- [ ] Comiteei com message clara
- [ ] Pushei para PR
- [ ] GitHub Actions passou
- [ ] Aprovei e mergesei

**Resultado:** CÃ³digo de produÃ§Ã£o excelente! ğŸš€

